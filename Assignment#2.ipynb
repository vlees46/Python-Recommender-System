{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03dc4e1d-54c6-47fa-a473-ab83b4dab8df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'goodreads_interactions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 114\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     goodreads \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoodreads_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(FN))\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m         handle,\n\u001b[0;32m    865\u001b[0m         ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m         newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m     )\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'goodreads_9k.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m     goodreads \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoodreads_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(FN))\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m     read_raw_data(NS, FN)\n\u001b[0;32m    117\u001b[0m     goodreads \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoodreads_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(FN))\n\u001b[0;32m    119\u001b[0m ratings_meta, metadata_lookup \u001b[38;5;241m=\u001b[39m merge_meta(\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_metadata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_id_map.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, goodreads)\n",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m, in \u001b[0;36mread_raw_data\u001b[1;34m(_num_samples, _fn)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_raw_data\u001b[39m(_num_samples, _fn):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    Reads and processes the raw Goodreads data.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    :param _num_samples: The number of rows to sample from the dataframe.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    :param _fn: Formatted filename of output.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    :return: None, saves the output dataframe.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     _df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoodreads_interactions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m_num_samples)\n\u001b[0;32m     23\u001b[0m     _df \u001b[38;5;241m=\u001b[39m _df[_df\u001b[38;5;241m.\u001b[39mis_read \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     24\u001b[0m     _df \u001b[38;5;241m=\u001b[39m _df[\u001b[38;5;241m0\u001b[39m:_num_samples]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'goodreads_interactions.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to build book recommendation systems.\n",
    "\"\"\"\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def read_raw_data(_num_samples, _fn):\n",
    "    \"\"\"\n",
    "    Reads and processes the raw Goodreads data.\n",
    "    :param _num_samples: The number of rows to sample from the dataframe.\n",
    "    :param _fn: Formatted filename of output.\n",
    "    :return: None, saves the output dataframe.\n",
    "    \"\"\"\n",
    "    _df = pd.read_csv(\"goodreads_interactions.csv\", nrows=_num_samples)\n",
    "    _df = _df[_df.is_read == 1]\n",
    "    _df = _df[0:_num_samples]\n",
    "    _df.to_csv('goodreads_{}.csv'.format(_fn, index=False))\n",
    "    \n",
    "\n",
    "def build_rating_matrix(_df):\n",
    "    \"\"\"\n",
    "    Converts a dataframe to a user-item interaction matrix.\n",
    "    :param _df: The input dataframe.\n",
    "    :return: Numpy matrix representing user-interaction ratings.\n",
    "    \"\"\"\n",
    "    _n_users = len(_df.user_id.unique()) + 1  # python indices start at zero, user_ids start at 1\n",
    "    _n_books = _df.book_idx.max() + 1  # python indices start at zero, book_ids start at 1\n",
    "    print('Users: {}'.format(_n_users))\n",
    "    print('Books: {}'.format(_n_books))\n",
    "    _ratings = np.zeros((_n_users, _n_books))\n",
    "    for _, row in tqdm(_df.iterrows()):\n",
    "        i = row.user_id\n",
    "        j = row.book_idx\n",
    "        _ratings[i, j] = row.rating\n",
    "        \n",
    "   # print(_ratings, 'THis is the rating matrix')\n",
    "    return _ratings\n",
    "\n",
    "\n",
    "def recommend_item_similarity(_matrix, _eps, _n_latent):\n",
    "    \"\"\"\n",
    "    Builds item similarities using truncated SVD.\n",
    "    :param _matrix: The user-item rating matrix.\n",
    "    :param _eps: The epsilon parameter for truncated SVD.\n",
    "    :param _n_latent: The number of latent features for truncated SVD.\n",
    "    :return: _sparse_features, The sparse matrix of item-similarity features.\n",
    "    \"\"\"\n",
    "    _item_svd = TruncatedSVD(n_components=_n_latent)\n",
    "    _item_features = _item_svd.fit_transform(_matrix.transpose())\n",
    "    print('Converting to sparse')\n",
    "    _sparse_features = sparse.csr_matrix(_item_features)\n",
    "    return _sparse_features\n",
    "\n",
    "\n",
    "def generate_similarity_matrix(_features, _metric):\n",
    "    \"\"\"\n",
    "    Generates the similarity matrix from either item or user features\n",
    "    based on the given similarity metric.\n",
    "    :param _features: The matrix of user or item features.\n",
    "    :param _metric: A string indicating which similarity metric should be used.\n",
    "    :return: _similarity_matrix, The final similarity matrix.\n",
    "    \"\"\"\n",
    "    assert _metric in ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']\n",
    "    print('Computing similarity')\n",
    "    _similarity_matrix = pairwise_distances(_features, metric=_metric)\n",
    "    return _similarity_matrix\n",
    "\n",
    "\n",
    "def merge_meta(_meta_path, _map_path, _ratings):\n",
    "    \"\"\"\n",
    "    Merges book metadata with ratings.\n",
    "\n",
    "    :param _meta_path: Path to book metadata csv.\n",
    "    :param _map_path: Path to book ID mapping.\n",
    "    :param _ratings: Dataframe of rating interactions.\n",
    "    :return: _ratings_meta, a dataframe of metadata and ratings and\n",
    "    _metadata_lookup, dictionary for the UI.\n",
    "    \"\"\"\n",
    "    _meta = pd.read_csv(_meta_path)\n",
    "    _map = pd.read_csv(_map_path)\n",
    "    _ratings_map = _ratings.merge(_map, how='left',\n",
    "                                  left_on='book_id', right_on='book_id_csv')\n",
    "    _ratings_map = _ratings_map[['user_id', 'book_id_csv', 'is_read',\n",
    "                                 'rating', 'is_reviewed', 'book_id_y']]\n",
    "    _ratings_map.columns = ['user_id', 'book_idx', 'is_read',\n",
    "                            'rating', 'is_reviewed', 'book_id']\n",
    "    _metadata_lookup = {}\n",
    "    for _, row in _ratings_map.iterrows():\n",
    "        _md = _meta[_meta['book_id'] == row['book_id']]\n",
    "        _metadata_lookup[str(row.book_idx)] = {\n",
    "            'title': _md['title'].values[0],\n",
    "            'link': _md['link'].values[0]}\n",
    "        \n",
    "    #print(_ratings_map, ' THis is the Rating MAP')\n",
    "    #print(_metadata_lookup, 'This is the metadata lookup')\n",
    "    return _ratings_map, _metadata_lookup\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NS = 9000\n",
    "    FN = '9k'\n",
    "    EPS = 1e-9\n",
    "    FACTORS = 4\n",
    "    METRIC = 'cityblock'\n",
    "    try:\n",
    "        goodreads = pd.read_csv('goodreads_{}.csv'.format(FN))\n",
    "    except FileNotFoundError:\n",
    "        read_raw_data(NS, FN)\n",
    "        goodreads = pd.read_csv('goodreads_{}.csv'.format(FN))\n",
    "        \n",
    "    ratings_meta, metadata_lookup = merge_meta(\n",
    "        'book_metadata.csv',\n",
    "        'book_id_map.csv', goodreads)\n",
    "    \n",
    "    print('Saving metadata')\n",
    "    \n",
    "    #print('This is the meataData',  ratings_meta)\n",
    "    #print(' THis is the metadata lookup', metadata_lookup)\n",
    "    \n",
    "    with open('books_metadata_{records}.json'.format(records=FN), 'w', encoding='utf-8') as m:\n",
    "        json.dump(metadata_lookup, m)\n",
    "        \n",
    "    ratings = build_rating_matrix(ratings_meta)\n",
    "    \n",
    "    #print(ratings,  'This is the RATING Matrix')\n",
    "    \n",
    "    item_features = recommend_item_similarity(ratings, EPS, FACTORS)\n",
    "    \n",
    "    #print(item_features,  'This is the Recommender similarity')\n",
    "    \n",
    "    sim = generate_similarity_matrix(item_features, METRIC)\n",
    "    \n",
    "    print(sim, 'This is the Generate similarity')\n",
    "    \n",
    "    print('Saving similarity')\n",
    "    with open('book_similarity_{factors}_{records}_{metric}.pkl'.format(factors=FACTORS,\n",
    "                                                                        records=FN,\n",
    "                                                                        metric=METRIC), 'wb') as f:\n",
    "        pickle.dump(sim, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652beb5f-5760-4dc2-966e-01ea9092bdfd",
   "metadata": {},
   "source": [
    "#### Test_Book Script #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9e2fa555-5f92-4925-9eca-5f6dfb7b5330",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 7.06813175 7.06813175 7.06813175]\n",
      " [0.         0.         0.         ... 7.06813175 7.06813175 7.06813175]\n",
      " [0.         0.         0.         ... 7.06813175 7.06813175 7.06813175]\n",
      " ...\n",
      " [7.06813175 7.06813175 7.06813175 ... 0.         0.         0.        ]\n",
      " [7.06813175 7.06813175 7.06813175 ... 0.         0.         0.        ]\n",
      " [7.06813175 7.06813175 7.06813175 ... 0.         0.         0.        ]]\n",
      "Loaded similarity\n",
      "Loaded metadata\n",
      "[[0.         0.         0.         ... 7.06813175 7.06813175 7.06813175]\n",
      " [0.         0.         0.         ... 7.06813175 7.06813175 7.06813175]\n",
      " [0.         0.         0.         ... 7.06813175 7.06813175 7.06813175]\n",
      " ...\n",
      " [7.06813175 7.06813175 7.06813175 ... 0.         0.         0.        ]\n",
      " [7.06813175 7.06813175 7.06813175 ... 0.         0.         0.        ]\n",
      " [7.06813175 7.06813175 7.06813175 ... 0.         0.         0.        ]]\n",
      "[ 6.01306756  6.01306756  6.01306756 ... 12.12342988 12.12342988\n",
      " 12.12342988]\n",
      "[1011, 999, 7351, 6960, 407]  this is res\n",
      "Searched for book: The Ultimate Hitchhiker's Guide: Five Complete Novels and One Story (Hitchhiker's Guide to the Galaxy, #1-5)\n",
      "Match 0: The Lovely Bones\n",
      "Match 1: The Handmaid's Tale\n",
      "Match 2: Night (The Night Trilogy #1)\n",
      "Match 3: Misery\n",
      "Match 4: 11/22/63\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to test recommender systems.\n",
    "\"\"\"\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "def test_recommender(_search, _similarity, _metadata):\n",
    "    \"\"\"\n",
    "    A function to test our recommender system.\n",
    "\n",
    "    :param _search: A book ID to search for.\n",
    "    :param _similarity: Our recommender similarity matrix.\n",
    "    :param _metadata: Mapping of book ID to title.\n",
    "    :return: List of titles of top 5 most similar books.\n",
    "    \"\"\"\n",
    "    print(_similarity)\n",
    "    row_sims = _similarity[_search, ]\n",
    "    print(row_sims)\n",
    "    #print(_metadata)\n",
    "    res = sorted(range(len(row_sims)), key=lambda sub: row_sims[sub])[-5:]\n",
    "    print(res, ' this is res')\n",
    "    \n",
    "    print('Searched for book: {sb}'.format(sb=_metadata[str(_search)]['title']))\n",
    "    for j, _ in enumerate(res):\n",
    "        print('Match {idx}: {book}'.format(idx=j, book=_metadata[str(res[j])]['title']))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #  Make sure these are updated to match your models from 'build_book_similarity.py'\n",
    "    NS = 9000\n",
    "    FN = '9k'\n",
    "    EPS = 1e-9\n",
    "    FACTORS = 4\n",
    "    METRIC = 'cityblock'\n",
    "\n",
    "    SIM_PATH = 'book_similarity_{factors}_{records}_{metric}.pkl'.format(factors=FACTORS,\n",
    "                                                                         records=FN,\n",
    "                                                                         metric=METRIC)\n",
    "    META_PATH = 'books_metadata_{records}.json'.format(records=FN)\n",
    "    with open(SIM_PATH, 'rb') as f:\n",
    "        sim = pickle.load(f)\n",
    "    print(sim)\n",
    "    print('Loaded similarity')\n",
    "    with open(META_PATH, 'r', encoding='utf-8') as m:\n",
    "        metadata_lookup = json.load(m)\n",
    "    print('Loaded metadata')\n",
    "    #  Try different indices for the first parameter of the following function\n",
    "    test_recommender(948, sim, metadata_lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98933579-9312-4678-884c-015ec9305f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
